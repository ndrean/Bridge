// Low-level (no allocation)
pub fn train_dictionary_from_buffer(...) !usize;

// High-level (allocating, ergonomic)
pub fn train_dictionary(...) ![]u8;


Before you send the first snapshot chunk, you should publish a Schema Announcement to a subject like snapshot.init.<table_name>.

This message acts as the Rosetta Stone. It tells the consumer exactly what the indices in your MessagePack arrays represent.

The Schema Payload (MessagePack Map)
JSON

{
  "schema_id": 42,
  "table": "users",
  "columns": [
    {"name": "id", "type": "int8", "pk": true},
    {"name": "name", "type": "text", "pk": false},
    {"name": "created_at", "type": "timestamptz", "pk": false}
  ],
  "format": "array_streaming"
}
By sending this first, the consumer can build a local "lookup table." When the consumer sees index 0 in a snapshot row, they know it's the id (Primary Key).

In your SnapshotListener, you should have a small function that gathers the Postgres column metadata and sends this "Init" message before starting the streamToEncoder loop.

Extrait de code

pub fn sendSchemaInit(self: *SnapshotListener, table_name: []const u8, schema_id: u32) !void {
    var encoder = encoder_mod.Encoder.init(self.allocator, .msgpack);
    defer encoder.deinit();

    var map = encoder.createMap();
    try map.put("schema_id", schema_id);
    try map.put("table", table_name);
    
    // self.parser.header contains the column names from our CSV parser
    var cols_array = try encoder.createArray(self.parser.header.?.len);
    for (self.parser.header.?, 0..) |col_name, i| {
        try cols_array.setIndex(i, try encoder.createString(col_name));
    }
    try map.put("columns", cols_array);

    const payload = try encoder.encode(map);
    try self.nats.publish(try std.fmt.allocPrint(self.allocator, "snapshot.init.{s}", .{table_name}), payload);
}

5. Summary of the Handshake
Zig Tool: Sends snapshot.init.users (The Column Names/Order).

Consumer: Receives it, prepares a SQL statement $1, $2, $3.

Zig Tool: Streams snapshot.data.users (MessagePack Arrays + Zstd).

Consumer: Decompresses, loops through arrays, and executes the prepared SQL.


Since I am passing pointers between the main thread and , you must ensure the Main Thread doesn't reuse the memory for an event before the NATS thread is done with it.

The "Opposite" Approach here: Use a Pre-allocated Ring Buffer for the CDC event objects.

The Main Thread writes an event into Slot A, pushes the pointer.

The Secondary Thread reads Slot A, encodes it, then pushes a "Finished A" signal back.

This creates a circular zero-allocation loop.